"""
Heart Disease Prediction - Model Training Script
Run this file to train all models and save them
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, confusion_matrix, classification_report,
                             roc_curve, roc_auc_score)
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
import warnings
warnings.filterwarnings('ignore')

# Create models directory if it doesn't exist
os.makedirs('models', exist_ok=True)
os.makedirs('images', exist_ok=True)

print("="*70)
print(" "*20 + "MODEL TRAINING STARTED")
print("="*70)

# ========================================
# 1. LOAD DATA
# ========================================
print("\n[1/6] Loading Dataset...")
df = pd.read_csv('data/heart.csv')
print(f"‚úì Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns")

# ========================================
# 2. PREPARE DATA
# ========================================
print("\n[2/6] Preparing Data...")

# Separate features and target
X = df.drop('target', axis=1)
y = df['target']

print(f"‚úì Features shape: {X.shape}")
print(f"‚úì Target shape: {y.shape}")

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"‚úì Training set: {X_train.shape[0]} samples")
print(f"‚úì Testing set: {X_test.shape[0]} samples")

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Save scaler
joblib.dump(scaler, 'models/scaler.pkl')
print("‚úì Scaler fitted and saved")

# ========================================
# 3. TRAIN MODELS
# ========================================
print("\n[3/6] Training Models...")

models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5),
    'SVM': SVC(kernel='rbf', probability=True, random_state=42)
}

trained_models = {}
results = {}

for name, model in models.items():
    print(f"\n  Training {name}...")
    
    # Train model
    model.fit(X_train_scaled, y_train)
    trained_models[name] = model
    
    # Make predictions
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    
    # Cross-validation score
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
    cv_mean = cv_scores.mean()
    
    results[name] = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'roc_auc': roc_auc,
        'cv_score': cv_mean,
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }
    
    # Save model
    joblib.dump(model, f'models/{name.lower().replace(" ", "_")}.pkl')
    
    print(f"  ‚úì {name} trained successfully")
    print(f"    Accuracy: {accuracy:.4f}")
    print(f"    F1-Score: {f1:.4f}")

print("\n‚úì All models trained and saved!")

# ========================================
# 4. MODEL COMPARISON
# ========================================
print("\n[4/6] Comparing Models...")

# Create comparison DataFrame
comparison_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': [results[m]['accuracy'] for m in results.keys()],
    'Precision': [results[m]['precision'] for m in results.keys()],
    'Recall': [results[m]['recall'] for m in results.keys()],
    'F1-Score': [results[m]['f1_score'] for m in results.keys()],
    'ROC-AUC': [results[m]['roc_auc'] for m in results.keys()],
    'CV Score': [results[m]['cv_score'] for m in results.keys()]
})

print("\n" + "="*70)
print("MODEL PERFORMANCE COMPARISON")
print("="*70)
print(comparison_df.to_string(index=False))
print("="*70)

# Save comparison
comparison_df.to_csv('models/model_comparison.csv', index=False)

# ========================================
# 5. VISUALIZATIONS
# ========================================
print("\n[5/6] Creating Visualizations...")

# 5.1 Model Comparison Bar Chart
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.ravel()

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'CV Score']
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#F7DC6F']

for idx, metric in enumerate(metrics):
    axes[idx].bar(comparison_df['Model'], comparison_df[metric], color=colors[idx])
    axes[idx].set_title(f'{metric} Comparison', fontweight='bold', fontsize=12)
    axes[idx].set_ylabel(metric)
    axes[idx].set_ylim([0, 1])
    axes[idx].tick_params(axis='x', rotation=45)
    
    # Add value labels on bars
    for i, v in enumerate(comparison_df[metric]):
        axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=9)

plt.tight_layout()
plt.savefig('images/model_comparison.png', dpi=300, bbox_inches='tight')
print("  ‚úì Model comparison chart saved")

# 5.2 Confusion Matrices
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.ravel()

for idx, (name, result) in enumerate(results.items()):
    cm = confusion_matrix(y_test, result['y_pred'])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],
                cbar=False, square=True)
    axes[idx].set_title(f'{name}\nConfusion Matrix', fontweight='bold')
    axes[idx].set_ylabel('Actual')
    axes[idx].set_xlabel('Predicted')

plt.tight_layout()
plt.savefig('images/confusion_matrices.png', dpi=300, bbox_inches='tight')
print("  ‚úì Confusion matrices saved")

# 5.3 ROC Curves
plt.figure(figsize=(10, 8))

for name, result in results.items():
    fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])
    auc = result['roc_auc']
    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2)

plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=2)
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curves - All Models', fontsize=16, fontweight='bold')
plt.legend(loc='lower right', fontsize=10)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('images/roc_curves.png', dpi=300, bbox_inches='tight')
print("  ‚úì ROC curves saved")

# 5.4 Feature Importance (Random Forest)
rf_model = trained_models['Random Forest']
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 8))
plt.barh(feature_importance['Feature'], feature_importance['Importance'], 
         color='skyblue')
plt.xlabel('Importance', fontsize=12)
plt.title('Feature Importance (Random Forest)', fontsize=16, fontweight='bold')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('images/feature_importance.png', dpi=300, bbox_inches='tight')
print("  ‚úì Feature importance chart saved")

# ========================================
# 6. DETAILED CLASSIFICATION REPORTS
# ========================================
print("\n[6/6] Generating Classification Reports...\n")

for name, result in results.items():
    print("="*70)
    print(f"{name} - Classification Report")
    print("="*70)
    print(classification_report(y_test, result['y_pred'], 
                                target_names=['No Disease', 'Disease']))

# Find best model
best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
best_accuracy = results[best_model_name]['accuracy']

print("\n" + "="*70)
print(" "*20 + "TRAINING COMPLETE!")
print("="*70)
print(f"\nüèÜ Best Model: {best_model_name}")
print(f"   Accuracy: {best_accuracy:.4f}")
print(f"\n‚úì All models saved in 'models/' directory")
print(f"‚úì All visualizations saved in 'images/' directory")
print(f"‚úì Model comparison saved as 'models/model_comparison.csv'")
print("\n" + "="*70)